{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to explore data a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the number of types :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Counts: {'water': 115, 'normal': 107, 'bug': 91, 'grass': 71, 'psychic': 56, 'fire': 51, 'electric': 49, 'rock': 44, 'dragon': 32, 'ground': 32, 'dark': 31, 'steel': 28, 'poison': 28, 'fighting': 27, 'ghost': 26, 'ice': 24, 'fairy': 17, 'flying': 4}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def count_classes(data_file):\n",
    "    \"\"\"\n",
    "    Counts the number of unique classes (types) in the dataset.\n",
    "\n",
    "    Args:\n",
    "        data_file (str): Path to the dataset file (CSV format).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with class types as keys and their counts as values.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(data_file)\n",
    "\n",
    "    # Count the occurrences of each class\n",
    "    class_counts = data[\"type\"].value_counts().to_dict()\n",
    "\n",
    "    return class_counts\n",
    "\n",
    "# Example usage\n",
    "data_file = \"data/data.csv\"  # Replace with your dataset's file path\n",
    "class_counts = count_classes(data_file)\n",
    "print(\"Class Counts:\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of types: 18\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of types:\",len(class_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the size of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Sizes and Counts: {(1280, 1280): 453, (431, 431): 258, (854, 854): 1, (1080, 1080): 27, (272, 272): 12, (405, 405): 1, (789, 789): 1, (1279, 1279): 1, (859, 859): 2, (899, 899): 3, (600, 600): 1, (944, 944): 1, (1029, 1029): 1, (399, 399): 1, (893, 893): 1, (950, 950): 1, (892, 892): 1, (937, 937): 4, (1043, 1043): 2, (1022, 1022): 1, (915, 915): 2, (864, 864): 1, (1127, 1127): 1, (850, 850): 1, (980, 980): 1, (931, 931): 1, (886, 886): 1, (664, 664): 1, (1261, 1261): 1, (1162, 1162): 1, (429, 429): 1, (860, 860): 1, (872, 872): 1, (766, 766): 1, (426, 426): 1, (834, 834): 2, (1137, 1137): 1, (957, 957): 1, (857, 857): 1, (1246, 1246): 1, (755, 755): 1, (862, 862): 3, (414, 414): 1, (1021, 1021): 1, (516, 516): 1, (594, 594): 1, (889, 889): 1, (418, 418): 1, (1032, 1032): 2, (956, 956): 1, (680, 680): 1, (878, 878): 1, (845, 845): 1, (856, 856): 1, (868, 868): 1, (1024, 1024): 1, (839, 839): 1, (869, 869): 2, (417, 417): 1, (410, 410): 1, (401, 401): 1, (970, 970): 1, (1125, 1125): 1, (848, 848): 1, (901, 901): 1, (902, 902): 1, (984, 984): 1, (955, 955): 1, (836, 836): 1, (946, 946): 1, (891, 891): 1, (910, 910): 1, (998, 998): 1, (769, 769): 1}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "def count_image_sizes(img_dir, data_file):\n",
    "    \"\"\"\n",
    "    Counts the sizes of all images in the dataset.\n",
    "\n",
    "    Args:\n",
    "        img_dir (str): Directory where images are stored.\n",
    "        data_file (str): Path to the CSV file containing image data.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with image sizes as keys and their counts as values.\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(data_file)\n",
    "\n",
    "    # Dictionary to store size counts\n",
    "    size_counts = {}\n",
    "\n",
    "    for img_name in data[\"image\"]:\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        if os.path.exists(img_path):\n",
    "            with Image.open(img_path) as img:\n",
    "                size = img.size  # (width, height)\n",
    "                if size not in size_counts:\n",
    "                    size_counts[size] = 1\n",
    "                else:\n",
    "                    size_counts[size] += 1\n",
    "\n",
    "    return size_counts\n",
    "\n",
    "# Example usage\n",
    "img_dir = \"data/images\"  # Replace with the actual image directory path\n",
    "data_file = \"data/data.csv\"  # Replace with the actual dataset file path\n",
    "sizes = count_image_sizes(img_dir, data_file)\n",
    "print(\"Image Sizes and Counts:\", sizes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will transform to the smallest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272, 272)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_smallest_image_size(img_dir, data_file):\n",
    "    \"\"\"\n",
    "    Finds the smallest image size in the dataset.\n",
    "\n",
    "    Args:\n",
    "        img_dir (str): Directory where images are stored.\n",
    "        data_file (str): Path to the CSV file containing image data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: The dimensions of the smallest image (width, height).\n",
    "    \"\"\"\n",
    "    # Load the dataset\n",
    "    data = pd.read_csv(data_file)\n",
    "\n",
    "    # Initialize minimum size\n",
    "    min_size = None\n",
    "\n",
    "    for img_name in data[\"image\"]:\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        if os.path.exists(img_path):\n",
    "            with Image.open(img_path) as img:\n",
    "                size = img.size  # (width, height)\n",
    "                if min_size is None or (size[0] * size[1] < min_size[0] * min_size[1]):\n",
    "                    min_size = size\n",
    "\n",
    "    return min_size\n",
    "\n",
    "smallest_size = find_smallest_image_size(img_dir, data_file)\n",
    "smallest_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des prédictions du modèle CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error initializing dataset or dataloader: __init__() got an unexpected keyword argument 'class_to_idx'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'class_to_idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 104\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m# Dataset and DataLoader\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 104\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCustomPKMNDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_to_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcls_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcustom_collate_fn)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset and DataLoader initialized successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'class_to_idx'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "from cnn import FlexibleCNN\n",
    "from dataset import CustomPKMNDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "from torchvision import transforms\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def load_model(model_path, num_classes, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Load the trained model from a .pth file.\n",
    "    \"\"\"\n",
    "    model = FlexibleCNN(input_channels=3, num_classes=num_classes)\n",
    "    try:\n",
    "        state_dict = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(state_dict)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        print(f\"Model loaded successfully from {model_path}.\")\n",
    "        return model\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        raise\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle variable-length sequences.\n",
    "    Returns:\n",
    "        images: Tensor of shape [batch_size, channels, height, width]\n",
    "        captions: LongTensor of shape [batch_size, max_seq_length]\n",
    "        type_indices: LongTensor of shape [batch_size]\n",
    "    \"\"\"\n",
    "    images = torch.stack([item[0] for item in batch])  # Stack all images\n",
    "    captions = [item[1] for item in batch]             # List of caption tensors\n",
    "    padded_captions = pad_sequence(captions, batch_first=True, padding_value=0)\n",
    "    type_indices = torch.tensor([item[2] for item in batch], dtype=torch.long)\n",
    "    return images, padded_captions, type_indices\n",
    "\n",
    "def show_predictions(model, dataloader, class_names, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Show a grid of images with their actual and predicted classes.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    try:\n",
    "        # Get a single batch: images, captions, and type_indices\n",
    "        images, _, type_indices = next(iter(dataloader))\n",
    "        images, labels = images.to(device), type_indices.to(device)\n",
    "\n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            probabilities = softmax(outputs, dim=1)\n",
    "            predictions = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "        # Convert images to PIL format for plotting\n",
    "        images = images.cpu()\n",
    "        images = [ToPILImage()(img) for img in images]\n",
    "\n",
    "        # Plot 16 images in a 4x4 grid\n",
    "        fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "        fig.suptitle(\"Actual vs Predicted Classes\", fontsize=16)\n",
    "\n",
    "        for idx, ax in enumerate(axes.flat):\n",
    "            if idx < len(images):\n",
    "                ax.imshow(images[idx])\n",
    "                ax.axis(\"off\")\n",
    "                actual_class = class_names[labels[idx].item()]\n",
    "                predicted_class = class_names[predictions[idx].item()]\n",
    "                ax.set_title(f\"A: {actual_class}\\nP: {predicted_class}\")\n",
    "            else:\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.subplots_adjust(top=0.9)\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during prediction: {e}\")\n",
    "        raise\n",
    "\n",
    "# Parameters\n",
    "img_dir = \"data/images\"\n",
    "data_file = \"data/data.csv\"\n",
    "batch_size = 16\n",
    "model_path = \"best_model_cnn.pth\"\n",
    "\n",
    "# Use the exact same class_names as used during training\n",
    "class_names = [\n",
    "    \"normal\", \"fire\", \"water\", \"grass\", \"electric\", \"ice\", \"fighting\", \"poison\", \"ground\",\n",
    "    \"flying\", \"psychic\", \"bug\", \"rock\", \"ghost\", \"dragon\", \"dark\", \"steel\", \"fairy\"\n",
    "]\n",
    "\n",
    "num_classes = len(class_names)\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((272, 272)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "try:\n",
    "    dataset = CustomPKMNDataset(\n",
    "        img_dir, \n",
    "        data_file, \n",
    "        transform=transform, \n",
    "        class_to_idx={cls_name: i for i, cls_name in enumerate(class_names)}\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    print(\"Dataset and DataLoader initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing dataset or dataloader: {e}\")\n",
    "    raise\n",
    "\n",
    "# Load model\n",
    "try:\n",
    "    model = load_model(model_path, num_classes, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    raise\n",
    "\n",
    "# Show predictions\n",
    "try:\n",
    "    show_predictions(model, dataloader, class_names, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying predictions: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same for CNN with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emac\\AppData\\Local\\Temp\\ipykernel_11644\\1189899858.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_model_cnn_with_attention.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 68\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Load model\u001b[39;00m\n\u001b[0;32m     67\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m4\u001b[39m  \u001b[38;5;66;03m# Adjust for padding or special tokens\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Show predictions\u001b[39;00m\n\u001b[0;32m     71\u001b[0m show_predictions(model, dataloader, class_names, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(model_path, num_classes, device)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03mLoad the trained model from a .pth file.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m FlexibleCNNWithAttention(input_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[1;32m----> 8\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\emac\\Documents\\M2\\DeepLearning\\dl_venv\\lib\\site-packages\\torch\\serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32mc:\\Users\\emac\\Documents\\M2\\DeepLearning\\dl_venv\\lib\\site-packages\\torch\\serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32mc:\\Users\\emac\\Documents\\M2\\DeepLearning\\dl_venv\\lib\\site-packages\\torch\\serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'best_model_cnn_with_attention.pth'"
     ]
    }
   ],
   "source": [
    "from cnn_with_attention import FlexibleCNNWithAttention\n",
    "\n",
    "def load_model(model_path, num_classes, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Load the trained model from a .pth file.\n",
    "    \"\"\"\n",
    "    model = FlexibleCNNWithAttention(input_channels=3, num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def show_predictions(model, dataloader, class_names, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Show a grid of images with their actual and predicted classes.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    images, labels, _ = next(iter(dataloader))  # Get a single batch\n",
    "    images, labels = images.to(device), labels[:, 0].to(device)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "        probabilities = softmax(outputs, dim=1)\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "    # Convert images to PIL format for plotting\n",
    "    images = images.cpu()\n",
    "    images = [ToPILImage()(img) for img in images]\n",
    "\n",
    "    # Plot 16 images in a 4x4 grid\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "    fig.suptitle(\"Actual vs Predicted Classes\", fontsize=16)\n",
    "\n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        if idx < len(images):\n",
    "            ax.imshow(images[idx])\n",
    "            ax.axis(\"off\")\n",
    "            actual_class = class_names[labels[idx].item()]\n",
    "            predicted_class = class_names[predictions[idx].item()]\n",
    "            ax.set_title(f\"A: {actual_class}\\nP: {predicted_class}\")\n",
    "        else:\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)\n",
    "    plt.show()\n",
    "\n",
    "# Parameters\n",
    "img_dir = \"data/images\"\n",
    "data_file = \"data/data.csv\"\n",
    "batch_size = 16\n",
    "model_path = \"best_model_cnn_with_attention.pth\"\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((272, 272)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomPKMNDataset(img_dir, data_file, transform=transform)\n",
    "class_names = dataset.vocab.itos  # Assuming vocab has an `itos` method for class names\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Load model\n",
    "num_classes = len(class_names) - 4  # Adjust for padding or special tokens\n",
    "model = load_model(model_path, num_classes, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Show predictions\n",
    "show_predictions(model, dataloader, class_names, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
